
@article{zahalka_ii-20_2021,
	title = {{II}-20: {Intelligent} and pragmatic analytic categorization of image collections},
	volume = {27},
	issn = {10772626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100415947&doi=10.1109%2fTVCG.2020.3030383&partnerID=40&md5=e2026d9ecd84238fa9f307f5e3ca7a2f},
	doi = {10.1109/TVCG.2020.3030383},
	abstract = {In this paper, we introduce 11-20 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11-20 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand ('fast-forward') the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II-20 is an intuitive, efficient, and effective multimedia analytics tool. © 2020 IEEE.},
	language = {English},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zahalka, Jan and Worring, Marcel and Van Wijk, Jarke J.},
	year = {2021},
	pmid = {33074815},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {Analytical models, Semantics, Image collections, Learning systems, Advanced visualizations, Flexible machines, Interactive learning, Machine classifications, Minimal interactions, Semantic interactions, Tight integrations, Turing machines},
	pages = {422 -- 431},
	file = {Zahalka et al. - 2021 - II-20 Intelligent and pragmatic analytic categori.pdf:C\:\\Users\\Richard\\Zotero\\storage\\SRMKI2TB\\Zahalka et al. - 2021 - II-20 Intelligent and pragmatic analytic categori.pdf:application/pdf},
}

@inproceedings{barthel_real-time_2019,
	title = {Real-time visual navigation in huge image sets using similarity graphs},
	isbn = {978-1-4503-6889-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074871490&doi=10.1145%2f3343031.3350599&partnerID=40&md5=3b5f194213bdd7c9dfaee6ce946256d5},
	doi = {10.1145/3343031.3350599},
	abstract = {Nowadays stock photo agencies often have millions of images. Nonstop viewing of 20 million images at a speed of 10 images per second would take more than three weeks. This demonstrates the impossibility to inspect all images and the difficulty to get an overview of the entire collection. Although there has been a lot of effort to improve visual image search, there is little research and support for visual image exploration. Typically, users start "exploring" an image collection with a keyword search or an example image for a similarity search. Both searches lead to long unstructured lists of result images. In earlier publications, we introduced the idea of graph-based image navigation and proposed an efficient algorithm for building hierarchical image similarity graphs for dynamically changing image collections. In this demo we showcase real-time visual exploration of millions of images with a standard web browser. Subsets of images are successively retrieved from the graph and displayed as a visually sorted 2D image map, which can be zoomed and dragged to explore related concepts. Maintaining the positions of previously shown images creates the impression of an "endless map". This approach allows an easy visual image-based navigation, while preserving the complex image relationships of the graph. © 2019 Association for Computing Machinery.},
	language = {English},
	booktitle = {{MM} 2019 - {Proceedings} of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Barthel, Kai Uwe and Hezel, Nico and Schall, Konstantin and Jung, Klaus},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Navigation, Visualization, Image retrieval, Similarity search, Image collections, Image enhancement, Flow visualization, Search engines, Visual exploration, Image similarity, Visual Navigation, Air navigation, Image graphs, Image navigation, Keyword search, Natural resources exploration},
	pages = {2202 -- 2204},
	file = {Barthel et al. - 2019 - Real-time visual navigation in huge image sets usi.pdf:C\:\\Users\\Richard\\Zotero\\storage\\CCUJX3GV\\Barthel et al. - 2019 - Real-time visual navigation in huge image sets usi.pdf:application/pdf},
}

@inproceedings{pogorelov_clustertag_2017,
	title = {{ClusterTag}: {Interactive} visualization, clustering and tagging tool for big image collections},
	isbn = {978-1-4503-4701-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021790565&doi=10.1145%2f3078971.3079018&partnerID=40&md5=a40e3cee0ea97511fcf7d884d855ec6a},
	doi = {10.1145/3078971.3079018},
	abstract = {Exploring and annotating collections of images without meta-data is a complex task which requires convenient ways of presenting datasets to a user. Visual analytics and information visualization can help users by providing interfaces, and in this paper, we present an open source application that allows users from any domain to use feature-based clustering of large image collections to perform explorative browsing and annotation. For this, we use various image feature extraction mechanisms, different unsupervised clustering algorithms and hierarchical image collection visualization. The performance of the presented open source software allows users to process and display thousands of images at the same time by utilizing heterogeneous resources such as GPUs and different optimization techniques. © 2017 Copyright held by the owner/author(s). ACM.},
	language = {English},
	booktitle = {{ICMR} 2017 - {Proceedings} of the 2017 {ACM} {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Pogorelov, Konstantin and Riegler, Michael and Halvorsen, Pål and Griwodz, Carsten},
	year = {2017},
	note = {Type: Conference paper},
	keywords = {Visualization, Clustering algorithms, Information systems, Software engineering, Open source software, Image processing, Annotation, Clustering, Open systems, Flow visualization, Program processors, Big collections, Image browsing, Open sources},
	pages = {112 -- 116},
	file = {Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\T29382RL\\Pogorelov et al. - 2017 - ClusterTag Interactive Visualization, Clustering .pdf:application/pdf},
}

@inproceedings{seah_prism_2014,
	title = {{PRISM}: {Concept}-preserving social image search results summarization},
	isbn = {978-1-4503-2259-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904564432&doi=10.1145%2f2600428.2609586&partnerID=40&md5=a064e5f4eaf72ce96d318d80d6f890ed},
	doi = {10.1145/2600428.2609586},
	abstract = {Most existing tag-based social image search engines present search results as a ranked list of images, which cannot be consumed by users in a natural and intuitive manner. In this paper, we present a novel concept-preserving image search results summarization algorithm named prism. prism exploits both visual features and tags of the search results to generate high quality summary, which not only breaks the results into visually and semantically coherent clusters but it also maximizes the coverage of the summary w.r.t the original search results. It first constructs a visual similarity graph where the nodes are images in the search results and the edges represent visual similarities between pairs of images. This graph is optimally decomposed and compressed into a set of concept-preserving subgraphs based on a set of summarization objectives. Images in a concept-preserving subgraph are visually and semantically cohesive and are described by a minimal set of tags or concepts. Lastly, one or more exemplar images from each subgraph is selected to form the exemplar summary of the result set. Through empirical study, we demonstrate the effectiveness of prism against state-of-the-art image summarization and clustering algorithms. Copyright 2014 ACM.},
	language = {English},
	booktitle = {{SIGIR} 2014 - {Proceedings} of the 37th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Seah, Boon-Siew and Bhowmick, Sourav S. and Sun, Aixin},
	year = {2014},
	note = {Type: Conference paper},
	keywords = {Clustering algorithms, Information retrieval, Flickr, Empirical studies, Image summarization, Visual feature, Search engines, Social image searches, Image search, Coherent clusters, Prisms, Visual similarity},
	pages = {737 -- 746},
	file = {Seah et al. - 2014 - PRISM Concept-preserving social image search resu.pdf:C\:\\Users\\Richard\\Zotero\\storage\\L5T8DUM8\\Seah et al. - 2014 - PRISM Concept-preserving social image search resu.pdf:application/pdf},
}

@article{phueaksri_approach_2023,
	title = {An {Approach} to {Generate} a {Caption} for an {Image} {Collection} {Using} {Scene} {Graph} {Generation}},
	volume = {11},
	issn = {21693536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177065586&doi=10.1109%2fACCESS.2023.3332098&partnerID=40&md5=5a49f0bd442bd9250d77ad742ad04f43},
	doi = {10.1109/ACCESS.2023.3332098},
	abstract = {Summarization is a challenging task that aims to generate a summary by grasping common information of a given set of information. Text summarization is a popular task of determining the topic or generating a textual summary of documents. In contrast, image summarization aims to find a representative summary of a collection of images. However, current methods are still restricted to generating a visual scene graph, tags, and noun phrases, but cannot generate a fitting textual description of an image collection. Thus, we introduce a novel framework for generating a summarized caption of an image collection. Since scene graph generation shows advancement in describing objects and their relationships on a single image, we use it in the proposed method to generate a scene graph for each image in an image collection. Then, we find common objects and their relationships from all scene graphs and represent them as a summarized scene graph. For this, we merge all scene graphs and select part of it by estimating the most common objects and relationships. Finally, the summarized scene graph is input into a captioning model. In addition, we introduce a technique to generalize specific words in the final caption into common concept words incorporating external knowledge. To evaluate the proposed method, we construct a dataset for this task by extending the annotation of the MS-COCO dataset using an image retrieval method. The evaluation of the proposed method on this dataset showed promising performance compared to text summarization-based methods. © ; 2023 The Authors.},
	language = {English},
	journal = {IEEE Access},
	author = {Phueaksri, Itthisak and Kastner, Marc A. and Kawanishi, Yasutomo and Komamizu, Takahiro and Ide, Ichiro},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Image edge detection, Task analysis, Semantics, Image analysis, Image retrieval, Text processing, Edge detection, Image captures, Image collection captioning, Image collections, Image summarization, Image-analysis, Job analysis, Multiple image, Multiple image summarization, Scene-graph summarization, Scene-graphs, Semantic summarization},
	pages = {128245 -- 128260},
	file = {Phueaksri et al. - 2023 - An Approach to Generate a Caption for an Image Col.pdf:C\:\\Users\\Richard\\Zotero\\storage\\BUSWG68N\\Phueaksri et al. - 2023 - An Approach to Generate a Caption for an Image Col.pdf:application/pdf},
}

@inproceedings{chavarro_visualizing_2013,
	title = {Visualizing multimodal image collections},
	isbn = {978-1-4799-1121-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891102120&doi=10.1109%2fSTSIVA.2013.6644945&partnerID=40&md5=bcc2753dabede9684990e6961564fdbe},
	doi = {10.1109/STSIVA.2013.6644945},
	abstract = {This paper presents two different strategies for visualizing multimodal image collections, which are based on a representation strategy that fuses text and visual content in the same latent space. This latent space allows to find semantic groups of images, which are used to select image prototypes to build a semantic visualization. The first strategy is a graph-based visualization in which edges represent image similarities and vertices represent images. The second is a multimodal visualization in which a set of image prototypes surround a semantic tag cloud. Thus, we built a system prototype in order to evaluate the strategies. Results show that the propose strategy is promising and it could be used in a real image exploration system to improve the image collection exploration process. © 2013 IEEE.},
	language = {English},
	booktitle = {Symposium of {Signals}, {Images} and {Artificial} {Vision} - 2013, {STSIVA} 2013},
	author = {Chavarro, Anyela and Camargo, Jorge and Gonzalez, Fabio A.},
	year = {2013},
	note = {Type: Conference paper},
	keywords = {Visualization, Semantics, Vision, Signal processing, Image collections, Multi-modal image, Latent factor analysis, Exploration systems, Graph-based visualization, Multi-modal visualization, Semantic visualization, summarization},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Richard\\Zotero\\storage\\VAZQ7NZT\\6644945.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\UBMAY7CU\\Chavarro et al. - 2013 - Visualizing multimodal image collections.pdf:application/pdf},
}

@article{camargo_kernel-based_2013,
	title = {A kernel-based framework for image collection exploration},
	volume = {24},
	issn = {1045926X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870864463&doi=10.1016%2fj.jvlc.2012.10.008&partnerID=40&md5=ba70e270d695f2040ced9389f44a55c6},
	doi = {10.1016/j.jvlc.2012.10.008},
	abstract = {While search engines have been a successful tool to search text information, image search systems still face challenges. The keyword-based query paradigm used to search in image collection systems, which has been successful in text retrieval, may not be useful in scenarios where the user does not have the precise way to express a visual query. Image collection exploration is a new paradigm where users interact with the image collection to discover useful and relevant pictures. This paper proposes a framework for the construction of an image collection exploration system based on kernel methods, which offers a mathematically strong basis to address each stage of an image collection exploration system: image representation, summarization, visualization and interaction. In particular, our approach emphasizes a semantic representation of images using kernel functions, which can be seamlessly harnessed across all system components. Experiments were conducted with real users to verify the effectiveness and efficiency of the proposed strategy. © 2012 Elsevier Ltd.},
	language = {English},
	number = {1},
	journal = {Journal of Visual Languages and Computing},
	author = {Camargo, Jorge E. and Caicedo, Juan C. and Gonzalez, Fabio A.},
	year = {2013},
	note = {Type: Article},
	pages = {53 -- 67},
	file = {Camargo et al. - 2013 - A kernel-based framework for image collection expl.pdf:C\:\\Users\\Richard\\Zotero\\storage\\HZ56HXSW\\Camargo et al. - 2013 - A kernel-based framework for image collection expl.pdf:application/pdf},
}

@article{van_der_corput_comparing_2017,
	title = {Comparing {Personal} {Image} {Collections} with {PICTuReVis}},
	volume = {36},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13188},
	doi = {10.1111/cgf.13188},
	abstract = {Digital image collections contain a wealth of information, which for instance can be used to trace illegal activities and investigate criminal networks. We present a method that enables analysts to reveal relations among people, based on the patterns in their collections. Similar temporal and spatial patterns can be found using a parameterized algorithm, visualization is used to choose the right parameters and to inspect the patterns found. The visualization shows relations between image properties: the person it belongs to, the concepts in the image, its time stamp and location. We demonstrate the method with image collections of 10, 000 people containing 460, 000 images in total.},
	language = {en},
	number = {3},
	urldate = {2024-01-22},
	journal = {Computer Graphics Forum},
	author = {Van Der Corput, Paul and Van Wijk, Jarke J.},
	month = jun,
	year = {2017},
	pages = {295--304},
	file = {Van Der Corput and Van Wijk - 2017 - Comparing Personal Image Collections with PICTuReV.pdf:C\:\\Users\\Richard\\Zotero\\storage\\DGB5ESLI\\Van Der Corput and Van Wijk - 2017 - Comparing Personal Image Collections with PICTuReV.pdf:application/pdf},
}

@article{radiano_story_2018,
	title = {Story {Albums}: {Creating} {Fictional} {Stories} {From} {Personal} {Photograph} {Sets}},
	volume = {37},
	issn = {0167-7055, 1467-8659},
	shorttitle = {Story {Albums}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13099},
	doi = {10.1111/cgf.13099},
	abstract = {We present a method for the automatic creation of ﬁctional storybooks based on personal photographs. Unlike previous attempts that summarize such collections by picking salient or diverse photos, or creating personal literal narratives, we focus on the creation of ﬁctional stories. This provides new value to users, as well as an engaging way for people (especially children) to experience their own photographs. We use a graph model to represent an artist-generated story, where each node is a ‘frame’, akin to frames in comics or storyboards. A node is described by story elements, comprising actors, location, supporting objects and time. The edges in the graph encode connections between these elements and provide the discourse of the story. Based on this construction, we develop a constraint satisfaction algorithm for one-to-one assignment of nodes to photographs. Once each node is assigned to a photograph, a visual depiction of the story can be generated in different styles using various templates. We show results of several ﬁctional visual stories created from different personal photo sets and in different styles.},
	language = {en},
	number = {1},
	urldate = {2024-01-22},
	journal = {Computer Graphics Forum},
	author = {Radiano, O. and Graber, Y. and Mahler, M. and Sigal, L. and Shamir, A.},
	month = feb,
	year = {2018},
	pages = {19--31},
	file = {Radiano et al. - 2018 - Story Albums Creating Fictional Stories From Pers.pdf:C\:\\Users\\Richard\\Zotero\\storage\\QMSBUVE8\\Radiano et al. - 2018 - Story Albums Creating Fictional Stories From Pers.pdf:application/pdf},
}

@article{zheng_album_2014,
	title = {Album {Quickview} in {Comic}-like {Layout} via {Quartet} {Analysis}},
	url = {http://diglib.eg.org/handle/10.2312/pgs.20141253.061-066},
	doi = {10.2312/PGS.20141253},
	abstract = {For clear summary and efﬁcient search of images for album, which carries a story of life record, we propose a new approach for quickview of album in comic-like layout via quartet analysis. How to organize the images in album and in what way to display images in collage are two key problems for album quickview. For the ﬁrst problem, we take the idea of model organization method based on quartet analysis to construct categorization tree to organize the images; while for the second problem, we utilize the topological structure of categorization tree to decompose it into multiple groups of images and extract representative image from each group for subsequent collage. For the collage part, we choose comic-like layout to present collage because comic provides a concise way for storytelling and it has variablitiy in layout styles, which is suitable for album summary. Experiments demonstrate that our method could organize the images effectively and present images in collage with diverse styles as well.},
	language = {en},
	urldate = {2024-01-22},
	journal = {Pacific Graphics Short Papers},
	author = {Zheng, Zhibin and Zhang, Yan and Miao, Zheng and Sun, Zhengxing},
	year = {2014},
	note = {Artwork Size: 6 pages
ISBN: 9783905674736
Publisher: The Eurographics Association},
	keywords = {General, I.4.0 [Computer Processing and Computer Vision], Image displays},
	pages = {6 pages},
	file = {Zheng et al. - 2014 - Album Quickview in Comic-like Layout via Quartet A.pdf:C\:\\Users\\Richard\\Zotero\\storage\\GARIK44R\\Zheng et al. - 2014 - Album Quickview in Comic-like Layout via Quartet A.pdf:application/pdf},
}
